"""Phase D: Isolation Forest baseline for LO2 aggregated features.

Loads the event-level Parquet generated by ``run_lo2_loader.py``, aggregates it to
(run, test_case, service) level, builds a sparse feature matrix, and trains an
Isolation Forest on the normal (``test_case == 'correct'``) aggregates. Scores are
written to disk so Phase F can attach explainability artefacts.
"""

import argparse
from pathlib import Path
from typing import Optional

import joblib
import numpy as np
import polars as pl
from scipy import sparse
from scipy.stats import rankdata
from sklearn.ensemble import IsolationForest
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import StandardScaler

GROUP_COLS = ["run", "test_case", "service"]


def load_events(events_path: Path) -> pl.DataFrame:
    if not events_path.exists():
        raise FileNotFoundError(f"Events Parquet not found: {events_path}")
    df_events = pl.read_parquet(events_path)
    if "seq_id" not in df_events.columns:
        raise ValueError("Expected 'seq_id' column is missing; run the loader with --save-parquet first.")
    return df_events


def aggregate_events(df_events: pl.DataFrame) -> pl.DataFrame:
    # Aggregate raw events per (run, test_case, service) and derive helper columns.
    df_agg = (
        df_events.group_by(GROUP_COLS)
        .agg(
            [
                pl.col("m_message").str.join(" ").alias("event_seq"),
                pl.len().alias("n_lines"),
                (pl.col("m_timestamp").max() - pl.col("m_timestamp").min())
                .dt.total_seconds()
                .fill_null(0.0)
                .alias("span_s"),
            ]
        )
        .with_columns((pl.col("test_case") != "correct").alias("y_error"))
        .select(GROUP_COLS + ["event_seq", "n_lines", "span_s", "y_error"])
        .sort(GROUP_COLS)
    )
    return df_agg


def maybe_join_metrics(df_agg: pl.DataFrame, metrics_path: Optional[Path]) -> pl.DataFrame:
    if not metrics_path:
        return df_agg

    if not metrics_path.exists():
        raise FileNotFoundError(f"Metrics Parquet not found: {metrics_path}")

    df_metrics = pl.read_parquet(metrics_path)
    required_cols = {"run", "test_case", "metric_name", "value"}
    missing = required_cols - set(df_metrics.columns)
    if missing:
        raise ValueError(f"Metrics file missing required columns: {sorted(missing)}")

    df_metric_agg = (
        df_metrics.group_by(["run", "test_case", "metric_name"])
        .agg(
            [
                pl.col("value").quantile(0.95).alias("p95"),
                pl.col("value").std().alias("std"),
            ]
        )
        .with_columns(
            pl.concat_str([pl.col("metric_name"), pl.lit("_p95")]).alias("metric_p95_col"),
            pl.concat_str([pl.col("metric_name"), pl.lit("_std")]).alias("metric_std_col"),
        )
    )

    df_metric_p95 = df_metric_agg.select(["run", "test_case", "metric_p95_col", "p95"]).pivot(
        values="p95",
        index=["run", "test_case"],
        columns="metric_p95_col",
    )

    df_metric_std = df_metric_agg.select(["run", "test_case", "metric_std_col", "std"]).pivot(
        values="std",
        index=["run", "test_case"],
        columns="metric_std_col",
    )

    df_metrics_wide = df_metric_p95.join(df_metric_std, on=["run", "test_case"], how="outer")
    df_joined = df_agg.join(df_metrics_wide, on=["run", "test_case"], how="left")
    return df_joined


def vectorize(
    df_features: pl.DataFrame,
    standardize: bool,
) -> tuple[sparse.csr_matrix, list[int], CountVectorizer, Optional[StandardScaler]]:
    # Build bag-of-words from event sequences and append numeric columns.
    event_text = df_features["event_seq"].to_list()
    if not event_text:
        raise ValueError("No event sequences available for vectorization.")

    vectorizer = CountVectorizer(token_pattern=r"\S+")
    bow_matrix = vectorizer.fit_transform(event_text)

    numeric_cols = [
        name
        for name in df_features.columns
        if name in {"n_lines", "span_s"} or name.endswith(("_p95", "_std"))
    ]

    numeric_frame = df_features.select(numeric_cols).fill_null(0) if numeric_cols else None
    numeric_data = numeric_frame.to_numpy() if numeric_frame is not None else None

    scaler: Optional[StandardScaler] = None
    if standardize and numeric_data is not None and numeric_data.size:
        scaler = StandardScaler()
        numeric_data = scaler.fit_transform(numeric_data)

    if numeric_data is not None and numeric_data.size:
        numeric_sparse = sparse.csr_matrix(numeric_data)
    else:
        numeric_sparse = sparse.csr_matrix((len(event_text), 0))

    feature_matrix = sparse.hstack([bow_matrix, numeric_sparse], format="csr")
    labels = df_features["y_error"].cast(pl.Int64).to_list()
    return feature_matrix, labels, vectorizer, scaler


def fit_isolation_forest(
    X: sparse.csr_matrix,
    labels: list[int],
    contamination: float,
    n_estimators: int,
    max_samples: Optional[int],
    random_state: int,
) -> IsolationForest:
    y = np.asarray(labels, dtype=int)
    train_mask = y == 0
    if train_mask.sum() == 0:
        raise ValueError("Training set is empty (no 'correct' aggregates).")

    model = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        max_samples=max_samples,
        random_state=random_state,
        n_jobs=-1,
    )
    model.fit(X[train_mask])
    return model


def build_scores_dataframe(
    df_features: pl.DataFrame,
    model: IsolationForest,
    X: sparse.csr_matrix,
    labels: list[int],
) -> pl.DataFrame:
    scores = -model.score_samples(X)  # Higher means more anomalous.
    ranks = rankdata(scores, method="dense").astype(int)
    y = np.asarray(labels, dtype=int)
    train_mask = (y == 0).astype(int)  # 1 if used for training (normal), else 0.

    df_scores = df_features.select(GROUP_COLS + ["y_error"]).with_columns(
        pl.Series("score_if", scores),
        pl.Series("rank_if", ranks),
        pl.Series("train_flag", train_mask),
    )
    return df_scores


def main() -> None:
    parser = argparse.ArgumentParser(description="Phase D Isolation Forest baseline for LO2 aggregates.")
    parser.add_argument(
        "--events",
        type=Path,
        default=Path("result/lo2/lo2_events.parquet"),
        help="Path to event-level Parquet produced by run_lo2_loader.py",
    )
    parser.add_argument(
        "--metrics",
        type=Path,
        default=None,
        help="Optional metrics Parquet to join before training.",
    )
    parser.add_argument(
        "--standardize",
        action="store_true",
        help="Standardize numeric columns before stacking.",
    )
    parser.add_argument("--contamination", type=float, default=0.1, help="IsolationForest contamination parameter.")
    parser.add_argument("--n-estimators", type=int, default=200, help="Number of trees for IsolationForest.")
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="max_samples for IsolationForest (None falls back to sklearn default).",
    )
    parser.add_argument("--random-state", type=int, default=42, help="Random state for reproducibility.")
    parser.add_argument(
        "--output-scores",
        type=Path,
        default=Path("result/lo2/lo2_if_scores.parquet"),
        help="Output path for scores (parquet or csv based on suffix).",
    )
    parser.add_argument(
        "--save-model",
        type=Path,
        default=None,
        help="Optional joblib path to persist (model, vectorizer, scaler).",
    )
    args = parser.parse_args()

    df_events = load_events(args.events)
    print(f"Loaded events: {df_events.shape}")

    df_agg = aggregate_events(df_events)
    print(f"Aggregated aggregates: {df_agg.shape}")

    df_features = maybe_join_metrics(df_agg, args.metrics)
    X, y, vectorizer, scaler = vectorize(df_features, args.standardize)

    print(f"Feature matrix shape: {X.shape}")
    print(f"Label distribution: positives={int(np.sum(y))} | total={len(y)}")

    model = fit_isolation_forest(
        X=X,
        labels=y,
        contamination=args.contamination,
        n_estimators=args.n_estimators,
        max_samples=args.max_samples,
        random_state=args.random_state,
    )
    print("Isolation Forest trained.")

    df_scores = build_scores_dataframe(df_features, model, X, y)
    top_rows = df_scores.sort("score_if", descending=True).head(5)
    print("Top-5 aggregates (descending IF score):")
    print(top_rows)

    output_path = args.output_scores
    output_path.parent.mkdir(parents=True, exist_ok=True)
    if output_path.suffix == ".csv":
        df_scores.write_csv(output_path)
    else:
        df_scores.write_parquet(output_path)
    print(f"Scores written to {output_path}")

    if args.save_model:
        args.save_model.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump((model, vectorizer, scaler), args.save_model)
        print(f"Persisted model artefacts to {args.save_model}")


if __name__ == "__main__":
    main()
