== 3.3 Pipeline-Architektur

Die für diese Arbeit entwickelte Pipeline-Architektur setzt auf dem generischen LogLead-Framework @mantyla_loglead_2024 auf und erweitert dieses um spezifische Komponenten für die Verarbeitung und Analyse des LO2-Datensatzes @bakhtin_lo2_2025. Die Implementierung folgt einer modularen Vier-Schichten-Architektur, die den kompletten Datenfluss von Roh-Logs über Feature-Engineering und Anomalieerkennung bis hin zur Erklärbarkeitsanalyse abdeckt. Im Zentrum der Evaluierung steht der `oauth2-token-Service`, da dieser im LO2-Datensatz @bakhtin_lo2_2025 die höchsten mittleren F1-Scores über verschiedene Fehlerszenarien hinweg aufweist und somit als repräsentatives Testobjekt für die Machbarkeit erklärbarer Anomalieerkennung dient.

=== 3.3.1 Loader-Ebene

Die Loader-Ebene bildet die Grundlage der Pipeline und ist in der Klasse `LO2Loader` (`loglead/loaders/lo2.py`) implementiert. Diese Klasse erweitert den abstrakten `BaseLoader` aus dem LogLead-Framework und übernimmt die Aufgabe, die hierarchisch strukturierten LO2-Rohdaten in ein standardisiertes, für die weitere Verarbeitung geeignetes Format zu überführen.

Der `LO2Loader` erwartet eine Verzeichnisstruktur, in der jeder experimentelle Run als separater Ordner organisiert ist. Innerhalb eines Run-Ordners befinden sich Unterverzeichnisse für verschiedene Testfälle: ein `correct`-Ordner mit fehlerfreien Ausführungen sowie mehrere Fehlerordner (z.B. `error_invalid_grant`, `error_unauthorized_client`), die jeweils spezifische Fehlerszenarien repräsentieren. Jeder Testfall-Ordner enthält Log-Dateien der beteiligten Microservices, benannt nach dem Schema `oauth2-oauth2-<service>.log`, wobei `<service>` Werte wie `client`, `code`, `token`, `refresh-token`, `key`, `service` oder `user` annehmen kann.

Die zentrale Methode `load()` traversiert die Verzeichnisstruktur systematisch und erzeugt für jede gefundene Log-Datei einen Polars-DataFrame mit den Spalten `m_message` (Log-Nachricht), `run` (Run-Identifikator), `test_case` (Testfall-Name), `service` (Microservice-Name) und `seq_id` (zusammengesetzter Sequenz-Identifikator aus Run, Testfall und Service). Diese Strukturierung entspricht dem ML-Sample-Schema des LO2-Papers @bakhtin_lo2_2025, bei dem jede Kombination aus `(run_id, test_name, service_name)` eine eigenständige Log-Sequenz mit zugehörigem Label darstellt.

Eine wesentliche LO2-spezifische Anpassung ist die service-basierte Filterung. Die Implementierung unterstützt die Parameter `single_service` und `service_types`, mit denen die Verarbeitung auf ausgewählte Services beschränkt werden kann. Intern verwendet der Loader Service-Filter der Form `oauth2-oauth2-<service>`, um nur relevante Log-Dateien zu laden. Diese Filterung stellt sicher, dass Logs nicht über Services hinweg konkateniert werden, sondern service-spezifische Sequenzen entstehen – ein fundamentales Designprinzip, das laut @bakhtin_lo2_2025 für die korrekte Modellierung und Evaluation unerlässlich ist.

Die Label-Generierung erfolgt automatisch durch Auswertung des `test_case`-Feldes: Sequenzen mit `test_case == "correct"` erhalten das Label `normal = True`, alle anderen Testfälle werden als `normal = False` markiert. Zusätzlich wird eine inverse `anomaly`-Spalte erzeugt. Das feingranulare Fehlerlabel bleibt im `test_case`-Feld erhalten und ermöglicht später differenzierte Analysen einzelner Fehlerklassen.

Ein weiteres wichtiges Feature des Loaders ist die Behandlung von Label-Leakage durch Initialisierungssequenzen. Der Parameter `trim_init_lines` (standardmäßig aktiviert) entfernt die ersten `init_lines_to_skip` Zeilen (Standard: 100 Zeilen) aus jedem Log, um Artefakte aus dem System-Start zu eliminieren, die Hinweise auf das spätere Verhalten enthalten könnten und somit die Evaluation verfälschen würden.

Die `_parse_timestamps()`-Methode extrahiert Zeitstempel aus den Log-Nachrichten mittels regulärer Ausdrücke (Format `HH:MM:SS.mmm`) und erzeugt eine `m_timestamp`-Spalte. Log-Einträge ohne Zeitstempel werden verworfen. Die Zeitstempel-Information wird für die spätere Berechnung von Sequenzdauern und zeitbasierten Features benötigt.

Nach dem Laden der Event-Daten erzeugt die `preprocess()`-Methode über die private Funktion `_create_df_seq()` einen aggregierten Sequenz-DataFrame `df_seq`, indem alle Events mit identischem `seq_id` gruppiert werden. Die Log-Nachrichten werden zu einer einzigen, durch Zeilenumbrüche getrennten Zeichenkette konkateniert, und zusätzliche Metadaten wie `start_time`, `end_time` und das `normal`-Label werden aus den Event-Daten übernommen.

Das Skript `demo/lo2_e2e/run_lo2_loader.py` dient als Wrapper für den `LO2Loader` und stellt eine umfangreiche CLI-Schnittstelle bereit, über die alle Parameter konfiguriert werden können. Die Persistierung erfolgt optional über das Flag `--save-parquet`, das sowohl Event-Daten (`lo2_events.parquet`) als auch Sequenz-Daten (`lo2_sequences.parquet` und `lo2_sequences_enhanced.parquet`) in das Ausgabeverzeichnis schreibt. Diese Parquet-Dateien bilden die Eingangsdaten für die nachfolgenden Pipeline-Phasen.

=== 3.3.2 Feature-Engineering (Enhancer)

Das Feature-Engineering erfolgt auf zwei Ebenen: der Event-Ebene durch die Klasse `EventLogEnhancer` (`loglead/enhancers/eventlog.py`) und der Sequenz-Ebene durch die Klasse `SequenceEnhancer` (`loglead/enhancers/sequence.py`). Beide Klassen transformieren und aggregieren Log-Daten, um aussagekräftige Features für die Anomalieerkennung zu extrahieren.

Der `EventLogEnhancer` operiert auf dem Event-DataFrame und erzeugt zusätzliche Spalten, die verschiedene Repräsentationen der Log-Nachrichten enthalten. Die Methode `words()` tokenisiert jede `m_message` mittels Leerzeichen-Trennung und erzeugt eine Liste von Wörtern (`e_words`) sowie deren Länge (`e_words_len`). Diese parserlose, auf einfacher Tokenisierung basierende Repräsentation ist schnell zu berechnen und erfasst die wesentlichen lexikalischen Elemente einer Log-Nachricht. Die Methode `trigrams()` erzeugt character-level Trigramme (`e_trigrams`, `e_trigrams_len`), indem der Text an drei versetzten Positionen in 3-Zeichen-Fragmente zerlegt wird. Trigrams erfassen lokale Muster und sind robust gegenüber kleinen Schreibvarianten oder Parametervariationen.

Für eine strukturiertere Repräsentation bietet der `EventLogEnhancer` parserbasierte Methoden wie `parse_drain()`, die den Drain-Algorithmus @he_drain_2017 anwenden, um Log-Templates zu extrahieren und jedem Event eine Template-ID (`e_event_drain_id`) zuzuweisen. Drain identifiziert wiederkehrende Strukturen in Logs, indem es variable Teile (z.B. IDs, Zeitstempel, numerische Werte) durch Platzhalter ersetzt. Die Implementierung unterstützt verschiedene Maskierungsmuster (IP-Adressen, Hexadezimalwerte, numerische Sequenzen), die in `masking_patterns_drain` definiert sind. Alternativ stehen weitere Parser wie Spell, LenMa, IPLoM oder Brain zur Verfügung, die in der LO2-Pipeline jedoch nicht standardmäßig verwendet werden.

Zusätzlich erzeugt die Methode `length()` numerische Features wie `e_chars_len` (Zeichenlänge der Log-Nachricht) und `e_lines_len` (Anzahl Zeilen bei mehrzeiligen Einträgen), die einfache quantitative Indikatoren für die Event-Komplexität liefern.

Der `SequenceEnhancer` aggregiert die Event-Features auf Sequenzebene und erzeugt dadurch Features, die eine gesamte Log-Sequenz charakterisieren. Die Methode `seq_len()` zählt die Anzahl Events pro `seq_id` und erzeugt die Spalte `seq_len` sowie ein Alias `e_event_id_len` für Kompatibilität mit der Token-Nomenklatur. Die Methode `duration()` berechnet die zeitliche Spanne zwischen dem frühesten und spätesten Zeitstempel einer Sequenz und legt die Ergebnisse in den Spalten `duration` und `duration_sec` (in Sekunden) ab. Diese zeitlichen Features sind besonders relevant für LO2, da Timeouts und Verzögerungen häufige Fehlerursachen in OAuth2-Flows darstellen.

Die Methode `tokens(token="e_words")` aggregiert die Tokenlisten aller Events einer Sequenz zu einer einzigen Liste auf Sequenzebene. Dies erfolgt durch Explodieren der Listen, Gruppierung nach `seq_id` und erneutes Aggregieren. Gleichzeitig wird die Gesamtzahl der Tokens (`e_words_len` auf Sequenzebene) durch Summierung der Event-Level-Längen berechnet. Dieselbe Operation kann für andere Tokentypen (z.B. `e_trigrams`) wiederholt werden, sodass verschiedene Repräsentationen parallel verfügbar sind.

Die in der LO2-Pipeline verwendete Standard-Enhancement-Konfiguration umfasst typischerweise die Erzeugung von `e_words`, `e_trigrams`, `e_event_drain_id` sowie die Sequenz-Features `seq_len`, `duration_sec`, `e_words_len` und `e_trigrams_len`. Diese Kombination ermöglicht sowohl die Verwendung parserlosen Ansätze (Words, Trigrams) als auch parserbasierter Repräsentationen (Drain-IDs) und bietet einen Vergleichsrahmen für die Evaluierung verschiedener Feature-Strategien.

Der praktische Trade-off zwischen parserlosen und parserbasierten Repräsentationen manifestiert sich in Geschwindigkeit, Robustheit und Interpretierbarkeit. Words und Trigrams sind schnell zu berechnen, robust gegenüber unbekannten Log-Formaten und erfordern keine Vorverarbeitung, erfassen aber syntaktische Variationen und Parameter-Wiederholungen direkt. Parserbasierte Repräsentationen wie Drain abstrahieren von variablen Teilen und erzeugen kompaktere Feature-Räume, sind jedoch rechenintensiver und können bei sehr heterogenen Logs zu einer Fragmentierung in viele kleine Template-Klassen führen. In den LO2-Experimenten werden beide Strategien parallel verfolgt, um ihre jeweiligen Stärken situationsabhängig nutzen zu können.

Die Enhancement-Phase wird im Skript `demo/lo2_e2e/LO2_samples.py` sowie im Loader-Wrapper `run_lo2_loader.py` orchestriert. Letzterer kann bei Verwendung des Flags `--save-enhancers` bereits die Standard-Enhancements anwenden und persistieren, sodass nachfolgende Phasen direkt auf `lo2_sequences_enhanced.parquet` zugreifen können.

=== 3.3.3 Anomalie-Detektor

Die Anomalie-Detektor-Schicht implementiert sowohl überwachte (supervised) als auch unüberwachte (unsupervised) Lernverfahren und ist primär in der Klasse `AnomalyDetector` (`loglead/anomaly_detection.py`) sowie im Orchestrierungsskript `demo/lo2_e2e/LO2_samples.py` realisiert. Die LO2-Pipeline nutzt eine Modell-Registry-Architektur, die eine flexible Auswahl und Kombination verschiedener Detektoren über eine zentrale Konfiguration ermöglicht.

Das Skript `LO2_samples.py` definiert in der Dictionary-Konstante `MODEL_REGISTRY` eine Vielzahl vorkonfigurierter Modellschlüssel, die jeweils eine spezifische Kombination aus Feature-Repräsentation, Lernalgorithmus und Hyperparametern beschreiben. Jeder Registrierungseintrag spezifiziert die zu verwendende Token-Spalte (`item_list_col`), optionale numerische Features (`numeric_cols`), die Trainingsmethode (`train_method`) sowie modellspezifische Hyperparameter (`train_kwargs`) und Vectorizer-Einstellungen (`vectorizer_kwargs`). Über das CLI-Argument `--models` kann zur Laufzeit eine Teilmenge dieser Modelle ausgewählt werden, was schnelle Experimente und Vergleiche ermöglicht.

Die Standard-Modell-Konfiguration der LO2-Pipeline umfasst:

- *Überwachte Modelle (Supervised):* `event_lr_words` (Logistic Regression auf Worttokens), `event_dt_trigrams` (Decision Tree auf Trigrams mit einer maximalen Tiefe von 8 und mindestens 10 Samples pro Blatt), `event_rf_words` (Random Forest mit 150 Bäumen, maximale Tiefe 12), `event_xgb_words` (XGBoost mit Histogram-basiertem Tree-Wachstum, 120 Bäume, maximale Tiefe 8), sowie numerische Varianten wie `sequence_lr_numeric` (Logistic Regression auf `seq_len` und `duration_sec`). Ein spezieller Eintrag `sequence_shap_lr_words` kennzeichnet ein Logistic-Regression-Modell, für das zusätzlich SHAP-Erklärungen generiert werden sollen.

- *Unüberwachte Modelle (Unsupervised):* `event_lof_words` (Local Outlier Factor trainiert ausschließlich auf normalen Sequenzen), `event_oneclass_svm_words` (One-Class SVM ebenfalls nur auf korrekten Runs), sowie der zentrale `IsolationForest`, der über separate CLI-Flags (`--if-contamination`, `--if-n-estimators`, `--if-max-samples`) gesteuert wird. Diese Modelle erfordern keine Labels im Training und sind für realitätsnahe Deployments relevant, in denen annotierte Anomalien nicht verfügbar sind.

Die Trainingsstrategie folgt dem in @bakhtin_lo2_2025 empfohlenen Ansatz, service-spezifische Modelle zu trainieren. Logs werden nicht über verschiedene Services hinweg konkateniert, sondern jede Kombination aus Run, Testfall und Service bildet eine eigenständige Sequenz. Für die Evaluation wird ein run-basierter Holdout-Split verwendet, der über die Funktion `_run_based_holdout_split()` implementiert ist. Diese Funktion gruppiert Sequenzen nach `(service, test_case)` und reserviert einen Anteil der Runs (standardmäßig 20%, konfigurierbar über `--sup-holdout-fraction`) als Holdout-Set. Der Split erfolgt entweder zufällig (`--sup-holdout-shuffle`) oder zeitlich, wobei die letzten Runs als Holdout dienen. Dieser run-basierte Split verhindert, dass zeitlich benachbarte Events desselben Runs in Training und Test gemischt werden, und spiegelt realistischere Deploymentszenarien wider.

Die Vektorisierung der Token-Features erfolgt über `CountVectorizer` aus Scikit-learn, der aus den Token-Listen Bag-of-Words-Vektoren erzeugt. Die Vectorizer-Konfiguration wird über `vectorizer_with_defaults()` aus `loglead/explainability_utils.py` zentralisiert und umfasst Parameter wie `max_features` (z.B. 5000 oder 40000 je nach Modell), `min_df` (Mindesthäufigkeit, typisch 5) sowie `dtype=np.uint16` zur Speicheroptimierung. Numerische Features werden als separate Spalten hinzugefügt und über `hstack` aus Scipy mit den vektorisierten Token-Features zu einer Gesamt-Feature-Matrix kombiniert.

Die Klasse `AnomalyDetector` bietet Trainingsmethoden wie `train_LR()`, `train_DT()`, `train_RF()`, `train_XGB()`, `train_LSVM()` für überwachte Modelle sowie `train_IsolationForest()`, `train_LOF()`, `train_OneClassSVM()` für unüberwachte Varianten. Jede Methode instanziiert das entsprechende Scikit-learn- oder XGBoost-Modell, trainiert es auf `X_train` und `labels_train` (bzw. nur `X_train` bei unüberwachten Modellen) und persistiert das trainierte Modell in `self.model`. Die `predict()`-Methode erzeugt Vorhersagen für `X_test` und gibt einen Polars-DataFrame zurück, der die Original-Sequenz-IDs mit den Vorhersagen (`pred_ano`) und optional Wahrscheinlichkeiten (`pred_ano_proba`) verknüpft. Gleichzeitig werden Accuracy, F1-Score und Confusion Matrix ausgegeben.

Ein besonderer Fokus der LO2-Pipeline liegt auf dem `oauth2-token-Service`, da dieser laut @bakhtin_lo2_2025 die höchsten mittleren F1-Scores über verschiedene Fehlerszenarien aufweist. Experimente können über `--service-types token` auf diesen Service eingeschränkt werden, um die Modellleistung in diesem kritischen Bereich zu maximieren.

Evaluationsmetriken werden über die Hilfsfunktionen in `demo/lo2_e2e/metrics_utils.py` berechnet und umfassen `precision_at_k()` (Precision unter den Top-k anomalsten Vorhersagen), `false_positive_rate_at_alpha()` (FPR bei einem definierten Alpha-Level) sowie `population_stability_index()` (PSI zur Detektion von Datenshift zwischen Training und Test). Diese Metriken werden als JSON- und CSV-Dateien im Ordner `demo/result/lo2/metrics/` persistiert und ermöglichen detaillierte Performance-Analysen und Vergleiche über verschiedene Modell-Konfigurationen hinweg.

Die Modellpersistierung erfolgt über Joblib. Bei Verwendung des Flags `--save-model models/lo2_if.joblib` wird das trainierte IsolationForest-Modell inklusive des angepassten Vectorizers als Bundle gespeichert. Überwachte Modelle können über `--save-sup-models <dir>` persistiert und später über `--load-sup-models <dir>` wiederverwendet werden, was die Trainingszeit in iterativen Explainability-Experimenten erheblich reduziert. Das Flag `--dump-metadata` erzeugt zusätzlich eine YAML-Datei (`models/model.yml`), die Git-Commit-Hash, Zeitstempel, Hyperparameter und weitere Metadaten für Reproduzierbarkeit dokumentiert.

=== 3.3.4 Erklärbarkeitsschicht (XAI)

Die Erklärbarkeitsschicht integriert zwei komplementäre Ansätze zur Interpretation von Anomalie-Vorhersagen: Nearest-Neighbour-basierte Vergleiche über die Klasse `NNExplainer` sowie SHAP-basierte Feature-Attributionen über die Klasse `ShapExplainer` (beide in `loglead/explainer.py`). Diese Schicht wird primär durch das Skript `demo/lo2_e2e/lo2_phase_f_explainability.py` orchestriert, das in @mantyla_loglead_2024 als LO2-Explainability-Hilfsskript bezeichnet wird.

Der `NNExplainer` ermittelt für jede als anomal klassifizierte Sequenz die ähnlichste normale Sequenz aus dem Trainingsdatensatz. Die Ähnlichkeit wird über Cosine-Similarity auf den Feature-Vektoren berechnet. Die Klasse initialisiert mit einem DataFrame, der die Vorhersagen enthält, sowie der zugehörigen Feature-Matrix `X` (als Dense- oder Sparse-Matrix). Die Methode `build_mapping()` spaltet die Daten in anomale und normale Teilmengen auf, berechnet die paarweisen Ähnlichkeiten und liefert ein Polars-DataFrame mit zwei Spalten: `anomalous_id` und `normal_id`, das die Zuordnung dokumentiert. Dieses Mapping wird als `<model>_nn_mapping.csv` im Explainability-Ordner persistiert und ermöglicht es Entwickelnden, für jede Anomalie ein konkretes Vergleichsbeispiel aus dem fehlerfreien Betrieb zu inspizieren. Die `print_log_content_from_nn_mapping()`-Methode gibt die tatsächlichen Log-Inhalte (als `e_words`-Listen) paarweise aus, was die manuelle Analyse unterstützt.

Der `ShapExplainer` generiert Shapley-Wert-basierte Erklärungen @lundberg_unified_2017, die quantifizieren, welchen Beitrag jedes Feature zur Anomalie-Entscheidung eines Modells leistet. Die Implementierung unterstützt mehrere SHAP-Verfahren: `shap.TreeExplainer` für baumbasierte Modelle (Decision Tree, Random Forest, XGBoost, IsolationForest), `shap.LinearExplainer` für lineare Modelle (Logistic Regression, LinearSVM) und `shap.KernelExplainer` als allgemeinen Fallback. Die Auswahl des Explainers erfolgt automatisch basierend auf dem Modelltyp, der über `train_method` in der Modell-Registry spezifiziert ist.

Die SHAP-Berechnung erfordert die Definition von Hintergrund-Samples (`background data`), die die typische Datenverteilung repräsentieren und als Referenz für die Shapley-Wertberechnung dienen. Das Skript `lo2_phase_f_explainability.py` bietet den Parameter `--shap-background`, der die Anzahl der Hintergrund-Samples steuert (Standard: 256). Bei Wert 0 wird das gesamte Trainingsset verwendet. Die Samples werden zufällig aus den normalen Sequenzen gezogen, um eine repräsentative Basis zu gewährleisten.

Zur Vermeidung von Speicher- und Laufzeitproblemen bei großen Feature-Räumen implementiert die Pipeline Guard-Rails. Der Parameter `--shap-feature-threshold` (Standard: 2000) begrenzt die Anzahl der Features, für die SHAP berechnet wird. Falls die Vectorizer-Dimensionalität diese Grenze überschreitet, wird eine Warnung ausgegeben und die SHAP-Berechnung für dieses Modell übersprungen. Ein zweiter Guard `--shap-cell-threshold` (Standard: 2000000) limitiert das Produkt aus Anzahl Test-Samples und Features. Beide Schranken sind über CLI-Argumente anpassbar und werden im SHAP-Konfigurationsobjekt dokumentiert, das mit den Explainability-Artefakten gespeichert wird.

Die `plot_shap()`-Funktion aus `loglead/explainability_utils.py` erzeugt Summary-Plots, die die einflussreichsten Features für alle anomalen Vorhersagen visualisieren. Die Funktion `save_top_features()` exportiert für jede anomale Sequenz die Top-N Features (nach absolutem SHAP-Wert) zusammen mit ihren Werten und Beiträgen in eine CSV-Datei (`<model>_shap_top_features.csv`). Diese Aufbereitung ermöglicht es Entwickelnden, gezielt nach wiederkehrenden Mustern über verschiedene Anomalien hinweg zu suchen.

Eine zusätzliche Aggregationsebene bietet die Gruppierung nach Fehlerlabel (`test_case`). Das Skript berechnet für jede Fehlerklasse die durchschnittlichen SHAP-Werte über alle betroffenen Sequenzen und identifiziert die charakteristischsten Features pro Fehlertyp. Diese Analyse wird in separaten CSV-Dateien (`<model>_shap_by_error.csv`) abgelegt und erleichtert die Fehlerdiagnose, indem sie typische Muster für spezifische Fehlerszenarien hervorhebt.

Für den IsolationForest, der ohne Labels trainiert wird, erfolgt eine zusätzliche False-Positive-Analyse. Alle im Testset als anomal klassifizierten, tatsächlich aber normalen Sequenzen werden in einer Textdatei (`if_false_positives.txt`) aufgelistet, zusammen mit ihren Anomalie-Scores und `seq_id`. Diese Information ist wertvoll für die Nachkalibrierung des Modells und die Justierung des Kontaminationsparameters.

Die Ausgabe der Explainability-Phase erfolgt in den Ordner `demo/result/lo2/explainability/<timestamp>/`, wobei jeder Lauf mit einem Zeitstempel versehen wird, um verschiedene Experimente zu archivieren. Die Struktur umfasst SHAP-Plots im PNG-Format, CSV-Tabellen mit Feature-Attributionen, NN-Mappings sowie Log-Dateien mit Konfigurations-Metadaten.

Das CLI des Explainability-Skripts ermöglicht umfangreiche Anpassungen: `--nn-source` steuert, welches Modell für das NN-Mapping verwendet wird (z.B. `if`, `sequence_shap_lr_words`), `--shap-sample` limitiert die Anzahl der Test-Samples für SHAP-Berechnungen, und `--skip-if` erlaubt es, die IsolationForest-Komponente zu überspringen, wenn nur überwachte Modelle analysiert werden sollen. Die Möglichkeit, vortrainierte Modelle über `--load-sup-models` zu laden, ermöglicht es, verschiedene SHAP-Konfigurationen zu testen, ohne das Training wiederholen zu müssen.

Die Integration von SHAP in die LO2-Pipeline folgt den Empfehlungen aus @mohale_systematic_2025 und @anthony_designing_2025, die SHAP als etabliertes Verfahren zur Erhöhung der Transparenz in IDS-Systemen identifizieren. Die Ausrichtung der Erklärungen auf Feature-Namen (Token, Sequenzlängen, Dauern) statt auf abstrakte Dimensionsindizes stellt sicher, dass die Erklärungen für Entwickelnde ohne Machine-Learning-Expertise nachvollziehbar sind. Die Kombination aus quantitativen SHAP-Werten und qualitativen Nearest-Neighbour-Vergleichen bietet einen ausgewogenen Ansatz, der sowohl statistische Evidenz als auch anschauliche Beispiele liefert.
